<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yansong Tang</title>
  
  <meta name="author" content="Yansong Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:820px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yansong Tang</name>
              </p>
              <p>I am a tenure-track Assistant Professor of <a href="https://www.tbsi.edu.cn/en/main.psp">Tsinghua-Berkeley Shenzhen Institute</a>, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, where I direct the IVG@SZ (Intelligent Vision Group at Shenzhen, the sister group of the <a href="http://ivg.au.tsinghua.edu.cn/"> IVG </a> at Beijing). Before that, I was a postdoctoral researcher at the Department of Engineering Science of the University of Oxford, working with <a href="https://www.robots.ox.ac.uk/~phst/"> Prof. Philip H. S. Torr</a> and <a href="https://www.robots.ox.ac.uk/~victor/"> Prof. Victor Prisacariu</a>. My research interests lie in computer vision. Currently, I am working in the fields of video analytics, vision-language understanding and 3D reconstruction.</p>
	      <p>I received my Ph.D degree with honour at Tsinghua University, advised by <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en"> Prof. Jie Zhou </a> and <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Prof. Jiwen Lu</a>, and B.S. degree in Automation from Tsinghua University. I have also spent time at <a href="https://www.microsoft.com/en-us/research/group/visual-computing/"> Visual Computing Group </a> of Microsoft Research Asia (MSRA), and <a href="http://www.stat.ucla.edu/~sczhu/"> Prof. Song-Chun Zhu</a>â€™s VCLA lab of University of California, Los Angeles (UCLA).</p>
	      <p>I am looking for self-motivated Master/PhD/Postdoc/RA. If you have top grades or coding skill, and are highly creative and interested in joining my group, please do not hesitate to send me your CV and transcripts of grades. Remote working is also welcome.</p>
	      <p style="text-align:center">
                <a href="data/email.txt"> Email </a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=TIbistUAAAAJ&hl=zh-CN"> Google Scholar </a> &nbsp/&nbsp
                <a href="https://github.com/AndyTang15"> GitHub </a> 
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
	      <img style="width:80%;max-width:80%" alt="profile photo" src="images/yansongtang.jpg">
            </td>
          </tr>
        </tbody></table>
		
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2022-04:</b> A talk at MSRA about <a href="data/LAVT-Yansong.pdf">LAVT</a>.
	      </li>
              <li style="margin: 5px;" >
                <b>2022-03:</b> Five papers to appear in CVPR 2022.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
			  <p>(*Equal Contribution, #Corresponding Author)</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/lavt.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LAVT: Language-Aware Vision Transformer for Referring Image Segmentation</papertitle>
              <br>
		    Zhao Yang*, Jiaqi Wang*, <strong>Yansong Tang#</strong>, Kai Chen, Hengshuang Zhao, Philip H.S. Torr
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2112.02244v2.pdf">[arxiv]</a>
	      <a href="https://github.com/yz93/LAVT-RIS">[code]</a>
              <br>
              <p> We present an end-to-end hierarchical Transformer-based network for referring segmentation.</p>
            </td>
          </tr>		

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BNV-Fusion.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion</papertitle>
              <br>
		    Kejie Li, <strong>Yansong Tang</strong>, Victor Adrian Prisacariu, Philip H.S. Torr
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2204.01139">[arxiv]</a>   
              <br>
              <p> We present Bi-level Neural Volume Fusion, which leverages recent advances in neural implicit representations and neural rendering for dense 3D reconstruction. In order to incrementally integrate new depth maps into a global neural implicit representation, we propose a novel bi-level fusion strategy that considers both efficiency and reconstruction quality by design.</p>
            </td>
          </tr>			
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/denseclip.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</papertitle>
              <br>
		    Yongming Rao*, Wenliang Zhao*, Guangyi Chen, <strong>Yansong Tang</strong>, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2112.01518">[arxiv]</a>
              <a href="https://github.com/raoyongming/DenseCLIP">[code]</a>  	    
              <br>
              <p> DenseCLIP is a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP.</p>
            </td>
          </tr>				
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TOMM.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning from Temporal Spatial Cubism for Cross-Dataset Skeleton-based Action Recognition</papertitle>
              <br>
			  <strong>Yansong Tang*</strong>, Xingyu Liu*, Xumin Yu, Danyang Zhang, Jiwen Lu, and Jie Zhou
              <br>
              <em>ACM Transactions on Multimedia Computing Communications and Applications (TOMM)</em>, 2022
	      <br>
              <a href="https://dl.acm.org/doi/full/10.1145/3472722">[pdf]</a>
              <a href="https://github.com/shanice-l/st-cubism">[code]</a>  	
              <br>
              <p> We devise a temporal-spatial Cubism strategy, which guides the network to be aware of the permutation of the segments in the temporal domain and the body parts in the spatial domain separately, thus improves the generalization ability of the model for cross-dataset action recognition.</p>
            </td>
          </tr>			
		
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/tpami.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2021
	      <br>
              <a href="https://arxiv.org/abs/2003.09392">[arXiv]</a>  
	      <a href="https://coin-dataset.github.io/">[Project Page]</a> 
              <br>
              <p> Journal version of the COIN dataset.</p>
            </td>
          </tr>			
		
         <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/stfc.PNG' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Breaking Shortcut: Exploring Fully Convolutional Cycle-Consistency for Video Correspondence Learning</papertitle>
              <br>
	           <strong>Yansong Tang*</strong>, Zhenyu Jiang*, Zhenda Xie*, Yue Cao, Zheng Zhang, Philip H. S. Torr, Han Hu
              <br>
              <em>ICCV SRVU workshop</em>, 2021
	      <br>
              <a href="https://arxiv.org/abs/2105.05838">[arxiv]</a>
              <a href="https://github.com/Steve-Tod/STFC3">[code]</a>  	    
              <br>
              <p> We observe a collapse phenomenon when directly applying fully convolutional cycle-consistency method for video correspondence learning, study the underline reason behind it, and propose a spatial transformation approach to address this issue.</p>
            </td>
          </tr>

		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BMVC21.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Hierarchical Interaction Network for Video Object Segmentation from Referring Expressions</papertitle>
              <br>
	           Zhao Yang*, <strong>Yansong Tang*</strong>, Luca Bertinetto, Hengshuang Zhao, Philip H. S. Torr
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2021
	      <br>
              <a>[arxiv]</a> 
              <a>[code] (to come)</a>  	    
              <br>
              <p> We present an end-to-end hierarchical interaction network for video object segmentation from referring expressions, which leverages the feature pyramid produced by the visual encoder to generate multiple levels of multi-modal features.</p>
            </td>
          </tr>
		
		
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/musdl.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Uncertainty-aware Score Distribution Learning for Action Quality Assessment</papertitle>
              <br>
			  <strong>Yansong Tang*</strong>, Zanlin Ni*, Jiahuan Zhou, Danyang Zhang, Jiwen Lu, Ying Wu, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
	      <br>
	      <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2006.07665">[arxiv]</a> 
              <a href="https://github.com/nzl-thu/MUSDL">[Code]</a>
              <br>
              <p> We propose an uncertainty-aware score distribution learning method and extend it to a multi-path model for action quality assessment.</p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/insight.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Graph Interaction Networks for Relation Transfer in Human Activity Videos</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Yi Wei, Xumin Yu, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2020
	      <br>
              <a href="data/GIN_paper.pdf">[PDF]</a>  
              <a>[code] (to come)</a>  		    
              <br>
              <p> We propose a graph interaction networks (GINs) model for transferring relation knowledge across two graphs two different scenarios for video
analysis, including a new proposed setting for unsupervised skeleton-based action recognition across different datasets, and supervised group activity recognition with multi-modal inputs.
		  </td>
          </tr>		
		
		
		    <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TIP19.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Semantics-Preserving Attention and Contextual Interaction for Group Activity Recognition</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Jiwen Lu, Zian Wang, Ming Yang, and Jie Zhou
              <br>
              <em>IEEE Transaction on Image Processing (TIP)</em>, 2019
              <br>
              <a href="data/TIP19b.pdf">[PDF]</a>  
	      <a href="data/TIP19b_supp.pdf">[Supp]</a>  
              <br>
              <p> We extend of our Semantics-Preserving Attention model with graph convolutional module for group activity recognition.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/coin.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis</papertitle>
              <br>
              <strong>Yansong Tang</strong>, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1903.02874">[arXiv]</a>  <a href="https://coin-dataset.github.io/">[Project Page]</a>  <a href="https://github.com/coin-dataset/annotation-tool">[Annotation Tool]</a> 
              <br>
              <p> COIN is one of the largest and most comprehensive instructional video analysis datasets with rich annotations. </p>
            </td>
          </tr>	  

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/mdnn.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Multi-stream Deep Neural Networks for RGB-D Egocentric Action Recognition</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Zian Wang, Jiwen Lu, Jianjiang Feng, and Jie Zhou
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2019
	      <br>
              <a href="data/TCSVT18_MDNN.pdf">[PDF]</a>  
	      <a href="http://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php">[Project Page]</a>
	      <a href="data/MDNN-TSN.tar.gz">[Code]</a>    
              <br>
              <p> We propose a multi-stream deep neural networks and THU-READ dataset for RGB-D egocentric action recognition.</p>
            </td>
          </tr>		
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/spts.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Mining Semantics-Preserving Attention for Group Activity Recognition</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Zian Wang, Peiyang Li, Jiwen Lu, Ming Yang, and Jie Zhou
              <br>
              <em>ACM Multimedia (MM)</em>, 2018
		    	      <br>
			  <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="data/MM18_SPTS.pdf">[PDF]</a>     
              <br>
              <p> We present a simple yet effective semantics-preserving attention module for group activity recognition.</p>
            </td>
          </tr>			
		
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/dprl.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition</papertitle>
              <br>
			  <strong>Yansong Tang*</strong>, Yi Tian*, Jiwen Lu, Peiyang Li, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018
              <a href="data/CVPR18_DPRL_paper.pdf">[PDF]</a>  
              <br>
              <p> We propose a simple yet effective method to select key frames for skeleton-based action recognition using the REINFORCE algorithm.</p>
            </td>
          </tr>
		
		
        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Honors and Awards</heading>
            <p>
			  <li style="margin: 5px;" >		    
		    Excellent Doctoral Dissertation Award of CAAI, 2021.
              </li>		    
			  <li style="margin: 5px;" >
                Excellent PhD Graduate of Beijing, 2020.
              </li>
			  <li style="margin: 5px;" >
                Excellent Doctoral Dissertation of Tsinghua University, 2020.
              </li>
			  <li style="margin: 5px;" >
                Zijing Scholar Fellowship for Prospective Researcher, Tsinghua University, 2020.
              </li>  
		    
		    
            </p>
          </td>
        </tr>
      </tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR 2019/2020, ICCV 2019, ECCV 2020, AAAI 2020/2021, ICME 2019/2020/2021, ICIP 2018/2019
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  TPAMI, TIP, TMM, TCSVT
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
<a href="https://clustrmaps.com/site/1bevc" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=pSqGxdyauRrVjsHq2YSTApeSzMqOewuzLgOt0g6Ow6Y&cl=ffffff" /></a>	          
       
      </td>
    </tr>
  </table>
</body>

</html>
