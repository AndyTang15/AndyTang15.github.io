<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yansong Tang</title>
  
  <meta name="author" content="Yansong Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:820px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yansong Tang</name>
              </p>
              <p>I am a tenure-track Assistant Professor of <a href="https://www.sigs.tsinghua.edu.cn/en">Shenzhen International Graduate School</a>, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, where I direct the IVG@SZ (Intelligent Vision Group at Shenzhen, the sister group of the <a href="http://ivg.au.tsinghua.edu.cn/"> IVG </a> at Beijing). Before that, I was a postdoctoral researcher at the Department of Engineering Science of the University of Oxford, working with <a href="https://torrvision.com/"> Prof. Philip H. S. Torr</a>. My current research interests lie in computer vision and pattern recognition.</p>
	      <p>I received my B.S. degree and Ph.D degree with honour from Tsinghua University, advised by <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en"> Prof. Jie Zhou </a> and <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Prof. Jiwen Lu</a>. I have also spent time at <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-Chun Zhu's</a> lab at University of California, Los Angeles (UCLA), and Microsoft Research Asia (MSRA), hosted by <a href="https://ancientmooner.github.io">Dr. Han Hu</a> and <a href="https://scholar.google.com/citations?user=P91a-UQAAAAJ&hl=en">Dr. Xin Tong</a> respectively.</p>
	      <p>I am looking for self-motivated Master/PhD/Postdoc. If you have top grades or coding skill, and are highly creative and interested in joining my group, please do not hesitate to send me your CV and transcripts of grades by <a href="data/email.txt">Email</a> after reading <a href="JD.pdf">this file</a>.</p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
	      <img style="width:80%;max-width:80%" alt="profile photo" src="images/yansongtang.jpg">
            </td>
          </tr>
        </tbody></table>
		
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" >
                <b>2025-01:</b> We are organizing the <a href="https://genbot-workshop.github.io/">"GenBot"</a> Workshop in ICLR 2025: Generative Models for Robot Learning.
	      </li>
	      <li style="margin: 5px;" >
                <b>2024-07:</b> We have organized <a href="https://venue-tutorial.github.io/">"VENUE"</a>, the only full-day tutorial in ECCV 2024: Recent Advances in Video Content Understanding and Generation.
	      </li>		    
              <li style="margin: 5px;" >
                <b>2024-06:</b> We have won the Long-form Video Question Answering Challenge of the CVPR2024 LOVEU Workshop.
	      </li>
              <li style="margin: 5px;" >
                <b>2024-06:</b> We have organized the <a href="https://mango-workshop.github.io/2024.html">"MANGO"</a> Workshop in CVPR 2024: New Trends in Multimodal Human Action Perception, Understanding and Generation. The recorded video will be released soon.
	      </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent Selected Publications  [<a style="font-size:20px;" href="https://scholar.google.com/citations?user=TIbistUAAAAJ&hl=zh-CN"> Full List </a>]</heading>
			  <p>(*Equal Contribution, #Corresponding Author)</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/yong-PAMI.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning High-Quality Dynamic Memory for Video Object Segmentation </papertitle>
              <br>
              Yong Liu, Ran Yu, Fei Yin, Xinyuan Zhao, Wei Zhao, Weihao Xia, Jiahao Wang, Yitong Wang, <strong>Yansong Tang#</strong>, and Yujiu Yang#
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2025
	      <br>
              <a href="https://doi.org/10.1109/TPAMI.2025.3532306">[Paper]</a>  
              <a href="https://github.com/yongliu20/QDMN?tab=readme-ov-file">[Code]</a> 	
              <br>
              <p>We propose QDMN, a memory-based dynamic framework for semi-supervised video object segmentation that performs adaptive quality awareness and dynamic update of reference information.</p>
            </td>
          </tr>		

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/lavt-pami.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Language-Aware Vision Transformer for Referring Segmentation</papertitle>
              <br>
              Zhao Yang*, Jiaqi Wang*, Xubing Ye*, <strong>Yansong Tang#</strong>, Kai Chen, Hengshuang Zhao, Philip H.S. Torr
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2024
	      <br>
              <a href="https://doi.org/10.1109/TPAMI.2024.3468640">[Paper]</a>  
              <a href="https://github.com/Yxxxb/LAVT-RS">[Code]</a> 		
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_LAVT_Language-Aware_Vision_Transformer_for_Referring_Image_Segmentation_CVPR_2022_paper.pdf">[Conference Version]</a> 
              <br>
              <p>We propose LAVT, a Transformer-based universal referring image and video segmentation (RIS and RVOS) framework that performs language-aware visual encoding in place of cross-modal fusion post feature extraction. </p>
            </td>
          </tr>		

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/tpami.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2021
	      <br>
              <a href="https://arxiv.org/abs/2003.09392">[arXiv]</a>  
              <a href="https://coin-dataset.github.io/">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/bYaAYkTj_0OxpDcWsToBWw">[中文解读]</a> 		
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Tang_COIN_A_Large-Scale_Dataset_for_Comprehensive_Instructional_Video_Analysis_CVPR_2019_paper.html">[Conference Version]</a>      
              <br>
              <p>COIN is currently the largest and most comprehensive instructional video analysis datasets with rich annotations. </p>
            </td>
          </tr>			

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/wizardmath.png" alt="dise">
            </td>
            <td width="75%" valign="center">
            <papertitle>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</papertitle>
            <br>
            Haipeng Luo*, Qingfeng Sun*, Can Xu#, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen#, <strong>Yansong Tang#</strong>, Dongmei Zhang
            <br>
            <em>The Thirteenth International Conference on Learning Representations (ICLR), 2025 </em>
            <br>
              <font color="red"><strong>Oral Presentation</strong></font>
            <br>
            <a href="https://arxiv.org/abs/2308.09583">[Paper]</a > 
            <a href="https://github.com/nlpxucan/WizardLM">[Code]</a>
            <p>We propose a new fully AI-powered automatic reinforcement learning method, Reinforcement Learning from Evol-Instruct Feedback (RLEIF), alongside Math Evol-Instruct and Process Supervision, for improving reasoning performance.</p >
          </td>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/thinkbot.png" alt="dise">
            </td>
            <td width="75%" valign="center">
            <papertitle>ThinkBot: Embodied Instruction Following with Thought Chain Reasoning</papertitle>
            <br>
            Guanxing Lu, Ziwei Wang#, Changliu Liu, Jiwen Lu and <strong>Yansong Tang#</strong>
            <br>
            <em>The Thirteenth International Conference on Learning Representations (ICLR),2025</em>
            <br>
            <a href="https://arxiv.org/abs/2312.07062">[Paper]</a > 
            <a href="https://guanxinglu.github.io/thinkbot/">[Project Page]</a > 
            <p>We have presented a ThinkBot agent that reasons the thought chain for missing instruction recovery in embodied instruction following (EIF) tasks.</p >
          </td>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/instarevive.png" alt="dise">
            </td>
            <td width="75%" valign="center">
            <papertitle>InstaRevive: One-Step Image Enhancement via Dynamic Score Matching</papertitle>
            <br>
            Yixuan Zhu, Haolin Wang, Ao Li, Wenliang Zhao, <strong>Yansong Tang</strong>, Jingxuan Niu, Lei Chen, Jie Zhou, Jiwen Lu
            <br>
            <em>The Thirteenth International Conference on Learning Representations (ICLR),2025</em>
            <br>
            <a href="https://openreview.net/pdf?id=G1CN7R5qwE">[Paper]</a > 
            <p>We propose InstaRevive, a straightforward yet powerful image enhancement framework that employs score-based diffusion distillation to harness potent generative capability and minimize the sampling steps.</p >
          </td>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/geolrm.png" alt="dise">
            </td>
            <td width="75%" valign="center">
            <papertitle>GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D Gaussian Generation</papertitle>
            <br>
            Chubin Zhang, Hongliang Song, Yi Wei, Chen Yu, Jiwen Lu, <strong>Yansong Tang#</strong>
            <br>
            <em>Thirty-eighth Conference on Neural Information Processing Systems, (NeurIPS), 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2406.15333">[arXiv]</a > 
            <a href="https://github.com/alibaba-yuanjing-aigclab/GeoLRM">[Code]</a >
            <a href="https://alibaba-yuanjing-aigclab.github.io/GeoLRM/">[Project Page]</a > 
            <p>This paper proposes a geometry-aware large reconstruction model for sparse-view reconstruction and 3D generation.</p >
          </td>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/q-vlm.png" alt="dise">
            </td>
            <td width="75%" valign="center">
            <papertitle>Q-VLM: Post-training Quantization for Large Vision-Language Models</papertitle>
            <br>
            Changyuan Wang, Ziwei Wang, Xiuwei Xu, <strong>Yansong Tang#</strong>, Jie Zhou, Jiwen Lu
            <br>
            <em>Thirty-eighth Conference on Neural Information Processing Systems, (NeurIPS), 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2410.08119">[arXiv]</a > 
            <a href="https://github.com/changyuanwang17/qvlm">[Code]</a >
            <p>This paper aims to achieve efficient inference and memory saving for large vision-language models.</p >
          </td>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/manigaussian.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
            <papertitle>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</papertitle>
            <br>
            Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu and <strong>Yansong Tang</strong>
            <br>
            <em>The European Conference on Computer Vision (ECCV), 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2403.08321">[arXiv]</a > 
            <a href="https://github.com/GuanxingLu/ManiGaussian">[Code]</a >
            <a href="https://guanxinglu.github.io/ManiGaussian">[Project Page]</a > 
            <p>We propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction.</p >
          </td>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/motionlcm.png" alt="dise">
            </td>
            <td width="75%" valign="center">
            <papertitle>MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model</papertitle>
            <br>
            Wenxun Dai, Linghao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, <strong>Yansong Tang</strong>
            <br>
            <em>The European Conference on Computer Vision (ECCV), 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2404.19759">[arXiv]</a > 
            <a href="https://github.com/Dai-Wenxun/MotionLCM">[Code]</a > 
            <a href="https://dai-wenxun.github.io/MotionLCM-page/">[Project Page]</a > 
            <p>We introduces MotionLCM, extending controllable motion generation to a real-time level.</p >
          </td>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pro-motion.png" alt="dise">
            </td>
            <td width="75%" valign="center">
            <papertitle>Plan, Posture and Go: Towards Open-Vocabulary Text-to-Motion Generation</papertitle>
            <br>
            Jinpeng Liu*, Wenxun Dai*, Chunyu Wang*, Yiji Cheng, <strong>Yansong Tang#</strong>, Xin Tong
            <br>
            <em>The European Conference on Computer Vision (ECCV), 2024</em>
            <br>
            <a href="https://arxiv.org/abs/2312.14828">[arXiv]</a > 
            <a href="https://moonsliu.github.io/Pro-Motion/">[Project Page]</a > 
            <p>We present a divide-and-conquer framework named PRO-Motion, which consists of three modules as motion planner, posture-diffuser and go-diffuser.</p >
          </td>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/segcap.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Segment and Caption Anything</papertitle>
              <br>
              Xiaoke Huang, Jianfeng Wang, <strong>Yansong Tang#</strong>, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, Zicheng Liu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2312.00869">[arXiv]</a>
              <a href="https://github.com/xk-huang/segment-caption-anything">[Code]</a>
              <a href="https://xk-huang.github.io/segment-caption-anything/">[Project Page]</a>
	      <a href="https://twitter.com/Tsinghua_Uni/status/1743950648092102855">[Tsinghua Twitter]</a>	    
              <br>
              <p> We propose a method to efficiently equip the Segment Anything Model (SAM) with the ability to generate regional captions by introducing a lightweight query-based feature mixer.</p>
            </td>
          </tr>	

		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/UniSeg.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Universal Segmentation at Arbitrary Granularity with Language Instruction</papertitle>
              <br>
              Yong Liu, Cairong Zhang, Yitong Wang, Jiahao Wang, Yujiu Yang, <strong>Yansong Tang#</strong>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2312.01623.pdf">[arXiv]</a>
              <a href="https://github.com/workforai/UniLSeg">[Code]</a> 
              <br>
              <p> We propose a unified framework to achieve universal segmentation at a wide spectrum of granularities and levels.</p>
            </td>
          </tr>	

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/openseg.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Open-Vocabulary Segmentation with Semantic-Assisted Calibration</papertitle>
              <br>
              Yong Liu*, Sule Bai*, Guanbin Li, Yitong Wang, <strong>Yansong Tang#</strong>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2312.04089.pdf">[arXiv]</a>
              <a href="https://github.com/workforai/SCAN">[Code]</a> 
              <br>
              <p> We propose an open-vocabulary segmentation (OVS) method by calibrating in-vocabulary and domain-biased embedding space with generalized contextual prior of CLIP. </p>
            </td>
          </tr>	

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/datafreequan.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Towards Accurate Data-free Quantization for Diffusion Models</papertitle>
              <br>
              Changyuan Wang, Ziwei Wang, Xiuwei Xu, <strong>Yansong Tang#</strong>, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <font color="red"><strong>Highlight</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2305.18723.pdf">[arXiv]</a>
	      <a href="https://github.com/ChangyuanWang17/APQ-DM">[Code]</a> 	    
              <br>
              <p> We propose an accurate data-free post-training quantization framework of diffusion models (ADP-DM) for efficient image generation. </p>
            </td>
          </tr>	
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/aqa-mi.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</papertitle>
              <br>
              Shiyi Zhang*, Sule Bai*, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, <strong>Yansong Tang#</strong>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2404.14471">[arXiv]</a>
              <a href="https://github.com/shiyi-zh0408/NAE_CVPR2024">[Code]</a>
              <br>
              <p> We investigate a new problem called narrative action evaluation (NAE) and propose a prompt-guided multimodal interaction framework. </p>
            </td>
          </tr>	

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/dpmesh.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery</papertitle>
              <br>
              Yixuan Zhu*, Ao Li*, <strong>Yansong Tang#</strong>, Wenliang Zhao, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2404.01424.pdf">[arXiv]</a>
              <a href="https://github.com/RammusLeo/DPMesh">[Code]</a> 
              <a href="https://rammusleo.github.io/dpmesh-proj/">[Project Page]</a> 
              <br>
              <p> We propose a new method to exploit diffusion priors for human mesh recovery (HMR) in occlusion and crowded scenarios. </p>
            </td>
          </tr>	


		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/flowie.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>FlowIE: Efficient Image Enhancement via Rectified Flow</papertitle>
              <br>
              Yixuan Zhu, Wenliang Zhao, Ao Li, <strong>Yansong Tang</strong>, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2406.00508">[arXiv]</a>
              <a href="https://github.com/EternalEvan/FlowIE">[Code]</a> 
              <br>
              <p> We proposed a unified framework for various efficient image enhancement tasks with generative diffusion priors. </p>
            </td>
          </tr>	
		

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/MCUFormer.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory</papertitle>
              <br>
              Yinan Liang, Ziwei Wang, Xiuwei Xu, <strong>Yansong Tang#</strong>, Jie Zhou, Jiwen Lu
              <br>
              <em>Thirty-seventh Conference on Neural Information Processing Systems, (<strong>NeurIPS</strong>)</em>, 2023.
              <br>
              <a href="https://arxiv.org/abs/2310.16898">[arXiv]</a>
              <a href="https://github.com/liangyn22/MCUFormer">[Code]</a> 
              <a href="https://mp.weixin.qq.com/s/j4-C2eDSpJPCAPp1XYwEZA">[中文解读]</a>		    
              <br>
              <p> We propose a hardware-algorithm co-optimizations method called MCUFormer to deploy vision transformers on microcontrollers with extremely limited memory, where we jointly design transformer architecture and construct the inference operator library to fit the memory resource constraint.</p>
            </td>
          </tr>	


		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Tem-adapter.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer</papertitle>
              <br>
              Guangyi Chen*, Xiao Liu*, Guangrun Wang, Kun Zhang, Philip H.S. Torr, Xiao-Ping Zhang, <strong>Yansong Tang#</strong>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2308.08414.pdf">[arXiv]</a>
              <a href="https://github.com/XLiu443/Tem-adapter">[Project Page]</a> 
              <br>
              <p> We present Tem-Adapter, a method that improves VQA by leveraging image-based knowledge and introducing temporal and semantic aligners.</p>
            </td>
          </tr>	

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/skip-plan.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning</papertitle>
              <br>
              Zhiheng Li, Wenjia Geng, Muheng Li, Lei Chen, <strong>Yansong Tang#</strong>, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2310.00608v1.pdf">[arXiv]</a>
              <a href="https://gitee.com/niujx66/skip_-plan-mindspore">[Code]</a> 
              <br>
              <p> We propose Skip-Plan, a condensed action space learning method for procedure planning in instructional videos.</p>
            </td>
          </tr>	
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/FLAG3D.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>FLAG3D: A 3D Fitness Activity Dataset with Language Instruction</papertitle>
              <br>
              <strong>Yansong Tang*</strong>, Jinpeng Liu*, Aoyang Liu*, Bin Yang, Wenxun Dai, Yongming Rao, Jiwen Lu, Jie Zhou, Xiu Li
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2212.04638.pdf">[arXiv]</a>
              <a href="https://andytang15.github.io/FLAG3D/">[Project Page]</a> 
              <br>
              <p> We present FLAG3D, a large-scale 3D fitness activity dataset with language instruction.</p>
            </td>
          </tr>			

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/LOGO.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LOGO: A Long-Form Video Dataset for Group Action Quality Assessment</papertitle>
              <br>
              Shiyi Zhang, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, <strong>Yansong Tang#</strong>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf">[PDF]</a>
              <a href="https://github.com/shiyi-zh0408/LOGO">[Project Page]</a> 
              <br>
              <p> LOGO is a new multi-person long-form video dataset for action quality assessment.</p>
            </td>
          </tr>			
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/hornet.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions</papertitle>
              <br>
              Yongming Rao*, Wenliang Zhao*, <strong>Yansong Tang</strong>, Jie Zhou, Ser-Nam Lim, Jiwen Lu
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2207.14284">[arXiv]</a>
              <a href="https://github.com/raoyongming/HorNet">[Code]</a>
              <a href="https://hornet.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/MyMIPv-bn9wVMLABurjOUA">[中文解读]</a> 
              <br>
              <p> HorNet is a family of generic vision backbones that perform explicit high-order spatial interactions based on Recursive Gated Convolution.</p>
            </td>
          </tr>		
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/OrdinalCLIP.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OrdinalCLIP: Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression</papertitle>
              <br>
	      Wanhua Li*, Xiaoke Huang*, Zheng Zhu, <strong>Yansong Tang</strong>, Xiu Li, Jiwen Lu, Jie Zhou
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2206.02338">[arXiv]</a>
              <a href="https://github.com/xk-huang/OrdinalCLIP">[Code]</a>
              <a href="https://xk-huang.github.io/OrdinalCLIP">[Project Page]</a> 
              <a href="https://zhuanlan.zhihu.com/p/565034693">[中文解读]</a> 
              <br>
              <p> We present a language-powered paradigm for ordinal regression.</p>
            </td>
          </tr>	
		
		
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/lavt.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LAVT: Language-Aware Vision Transformer for Referring Image Segmentation</papertitle>
              <br>
		    Zhao Yang*, Jiaqi Wang*, <strong>Yansong Tang#</strong>, Kai Chen, Hengshuang Zhao, Philip H.S. Torr
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2112.02244v2.pdf">[arxiv]</a>
	      <a href="https://github.com/yz93/LAVT-RIS">[code]</a>
	      <a href="https://mp.weixin.qq.com/s/k8IgExjxybRSscoRy-jHSg">[中文解读]</a> 	    
              <br>
              <p> We present an end-to-end hierarchical Transformer-based network for referring segmentation.</p>
            </td>
          </tr>		

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BNV-Fusion.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion</papertitle>
              <br>
		    Kejie Li, <strong>Yansong Tang</strong>, Victor Adrian Prisacariu, Philip H.S. Torr
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2204.01139">[arxiv]</a>   
    	      <a href="https://github.com/likojack/bnv_fusion">[code]</a>
	      <a href="https://mp.weixin.qq.com/s/h-8O1kFxX_IVzFB6bCNrvg">[中文解读]</a> 	    
              <br>
              <p> We present Bi-level Neural Volume Fusion, which leverages recent advances in neural implicit representations and neural rendering for dense 3D reconstruction. In order to incrementally integrate new depth maps into a global neural implicit representation, we propose a novel bi-level fusion strategy that considers both efficiency and reconstruction quality by design.</p>
            </td>
          </tr>			
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/denseclip.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</papertitle>
              <br>
		    Yongming Rao*, Wenliang Zhao*, Guangyi Chen, <strong>Yansong Tang</strong>, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
	      <a href="https://arxiv.org/abs/2112.01518">[arXiv]</a>
              <a href="https://github.com/raoyongming/DenseCLIP">[Code]</a>
              <a href="https://denseclip.ivg-research.xyz">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/fERXjGBVXzo6TaYiV2Z9ZQ">[中文解读]</a> 	    
              <br>
              <p> DenseCLIP is a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP.</p>
            </td>
          </tr>				

	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/musdl.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Uncertainty-aware Score Distribution Learning for Action Quality Assessment</papertitle>
              <br>
			  <strong>Yansong Tang*</strong>, Zanlin Ni*, Jiahuan Zhou, Danyang Zhang, Jiwen Lu, Ying Wu, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
	      <br>
	      <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2006.07665">[arxiv]</a> 
              <a href="https://github.com/nzl-thu/MUSDL">[Code]</a>
              <br>
              <p> We propose an uncertainty-aware score distribution learning method and extend it to a multi-path model for action quality assessment.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Dataset</heading>
            <p>
	      <li style="margin: 5px;"> 
              COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis. <a href="https://coin-dataset.github.io/"> [Website] </a>  <a href="https://coin-dataset.github.io/"> [Download] </a> 
              </li>      
	      <li style="margin: 5px;"> 
              FLAG3D: A 3D Fitness Activity Dataset with Language Instruction. <a href="https://andytang15.github.io/FLAG3D/"> [Website] </a>  <a href="https://d2c365sdqy.feishu.cn/share/base/form/shrcnmEaSaf2qzC5gZTIHlPMR4g"> [Download] </a> 
              </li>  
        <li style="margin: 5px;"> 
              THU-READ: Tsinghua University RGB-D Egocentric Action Dataset. <a href="https://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php"> [Website] </a>  <a href="https://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php"> [Download] </a> 
              </li>    
	      <li style="margin: 5px;"> 
              LOGO: A Long-Form Video Dataset for Group Action Quality Assessment. <a href="https://github.com/shiyi-zh0408/LOGO"> [Website] </a>  <a href="https://github.com/shiyi-zh0408/LOGO"> [Download] </a> 
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>	

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching</heading>
            <p>
	      <li style="margin: 5px;"> 
                  Reinforcement Learning, Spring 2024
              </li>      
	      <li style="margin: 5px;"> 
                  Deep Learning: Frontier and Interdisciplinary Research, Fall 2023
              </li>      
	      <li style="margin: 5px;"> 
                  Data Mining: Theory and Algorithms, Fall 2022 (with <a href="https://scholar.google.com/citations?user=Ha8rlUgAAAAJ&hl=en"> Prof. Xinlei Chen</a>)
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>	      
	      
	      
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Honors and Awards</heading>
            <p>
		<li style="margin: 5px;" >		    
		    Guangdong Natural Science Funds for Distinguished Young Scholar, 2025.
              </li>	
		    <li style="margin: 5px;" >		    
		    Startrack Program by MSRA, 2023.
              </li>	
	      <li style="margin: 5px;" >		    
		    Young Elite Scientist Sponsorship Program by CAST, 2022.
              </li>		    		    
			  <li style="margin: 5px;" >		    
		    Excellent Doctoral Dissertation Award of CAAI, 2021.
              </li>		    
			  <li style="margin: 5px;" >
                Excellent PhD Graduate of Beijing, 2020.
              </li>
			  <li style="margin: 5px;" >
                Excellent Doctoral Dissertation of Tsinghua University, 2020.
              </li>
			  <li style="margin: 5px;" >
                Zijing Scholar Fellowship for Prospective Researcher, Tsinghua University, 2020.
              </li>  
            </p>
          </td>
        </tr>
      </tbody></table>

	      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Group</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>PhD students:</b>
	              <table  border=2 width = 800  bordercolor="white" align="left">
                  <tr>
                  <td width= "400"><a href="https://nonozhizhiovo.github.io">Zhiheng Li</a> (2021-; with <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>)</td>
                  <td width= "400"><a href="https://eternalevan.github.io/">Yixuan Zhu</a> (2022-; with <a href="https://eternalevan.github.io/">Prof. Jie Zhou</a>)</td>
                  </tr>
		  <tr>
                  <td width= "400"><a href="https://yongliu20.github.io/">Yong Liu</a> (2023-)</td>
                  <td width= "400"><a href="https://shiyi-zh0408.github.io/">Shiyi Zhang</a> (2023-)</td>
                  </tr>	  
		  <tr>
                  <td width= "400">Xin Dong (2023-;with <a href="https://www.pcl.ac.cn/">Pengcheng Lab</a>)</td>
                  <td width= "400">Chubin Zhang (2024-)</td>
                  </tr>	  
                  <tr>
                    <td width= "400"><a href="https://changyuanwang17.github.io/">Changyuan Wang</a> (2024-)</td>
                    <td width= "400">Haipeng Luo (2024-;with <a href="https://www.pcl.ac.cn/">Pengcheng Lab</a>)</td>
                    </tr>	  		      
                </table> 
              </li>
              <li style="margin: 5px;"> 
                <b>Master students (who have their homepages):</b>
                <table  border=2   width = 800 bordercolor="white" align="left">
                  <tr>
                  <td width= "400"><a href="https://moonsliu.github.io/">Jinpeng Liu</a> (2022-)</td>
                  <td width= "400"><a href="https://guanxinglu.github.io/">Guanxing Lu</a> (2023-)</td>
                  </tr>
                  <tr>				  
		  <td width= "400"><a href="https://github.com/Dai-Wenxun">Wenxun Dai</a> (2023-)</td>
      <td width= "400"><a href="https://sulebai.github.io/">Sule Bai</a> (2023-)</td>
		  </tr>
		  <tr>				  
      <td width= "400"><a href="https://zhang9302002.github.io/">Haoji Zhang</a> (2024-)</td>

      <td width= "400"><a href="https://rammusleo.github.io/">Ao Li</a> (2024-)</td>
		  </tr>	
          </table> 
	       </li>		    
            </p>
          </td>
        </tr>
      </tbody></table>	      
	    
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Alumni</heading>
          <p>
              <table  border=2 width = 800  bordercolor="white" align="left">
                <tr>
                  <td width= "400"><a href="https://xk-huang.github.io">Xiaoke Huang </a> (2022-2024) </td>
                </tr> 	      
              </table> 
          </p>
        </td>
      </tr>
    </tbody></table>	      
	      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Associate Editor:</b> JVCI
              </li>
              <li style="margin: 5px;"> 
                <b>Area Chair:</b> CVPR 2025, FG 2023
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR, ICCV, ECCV, AAAI and so on
              </li>		    
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  TPAMI, TIP, TMM, TCSVT and so on
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
<a href="https://clustrmaps.com/site/1bevc" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=pSqGxdyauRrVjsHq2YSTApeSzMqOewuzLgOt0g6Ow6Y&cl=ffffff" /></a>	          
       
      </td>
    </tr>
  </table>
</body>

</html>
