<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yansong Tang</title>
  
  <meta name="author" content="Yansong Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:820px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yansong Tang</name>
              </p>
              <p>I am currently a Sponsored Postdoctoral Researcher in the Department of Engineering Science at the University of Oxford, working with <a href="https://www.robots.ox.ac.uk/~phst/"> Prof. Philip H. S. Torr</a>. My research interests lie in computer vision. Currently, I am working towards learning data-efficient and interpretable representations for video understanding.</p>
	      <p>Prior to that, I received my Ph.D degree at Tsinghua University, advised by <a href="http://www.au.tsinghua.edu.cn/info/1078/1635.htm"> Prof. Jie Zhou </a> and <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Prof. Jiwen Lu</a>, and B.S. degree in Automation from Tsinghua University. I have also spent time at <a href="https://www.microsoft.com/en-us/research/group/visual-computing/"> Visual Computing Group </a> of Microsoft Research Asia (MSRA), and <a href="http://www.stat.ucla.edu/~sczhu/"> Prof. Song-Chun Zhu</a>â€™s VCLA lab of University of California, Los Angeles (UCLA).</p>
              <p style="text-align:center">
                <a href="mailto:yansong.tang@eng.ox.ac.uk"> Email </a> &nbsp/&nbsp
                <a href="data/CV.pdf"> CV </a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=TIbistUAAAAJ&hl=zh-CN"> Google Scholar </a> &nbsp/&nbsp
                <a href="https://github.com/AndyTang15"> GitHub </a> &nbsp/&nbsp
		<a href="data/Research_statement.pdf"> Research Statement </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
	      <img style="width:80%;max-width:80%" alt="profile photo" src="images/yansongtang.jpg">
            </td>
          </tr>
        </tbody></table>
		
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2020-08:</b> I was awarded Excellent PhD Graduate of Beijing.
              </li>
              <li style="margin: 5px;" >
                <b>2020-07:</b> My Ph.D dissertation was awarded Excellent Doctoral Dissertation of Tsinghua University. 
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
			  <p>* indicates equal contribution</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/stfc.PNG' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Breaking Shortcut: Exploring Fully Convolutional Cycle-Consistency for Video Correspondence Learning</papertitle>
              <br>
	           <strong>Yansong Tang*</strong>, Zhenyu Jiang*, Zhenda Xie*, Yue Cao, Zheng Zhang, Philip H. S. Torr, Han Hu
              <br>
              <em>Technical Report</em>, 2021
	      <br>
              <a href="https://arxiv.org/abs/2105.05838">[arxiv]</a> 
              <br>
              <p> We observe a collapse phenomenon when directly applying fully convolutional cycle-consistency method for video correspondence learning, study the underline reason of it, and propose a spatial transformation approach to address this issue.</p>
            </td>
          </tr>
		
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/musdl.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Uncertainty-aware Score Distribution Learning for Action Quality Assessment</papertitle>
              <br>
			  <strong>Yansong Tang*</strong>, Zanlin Ni*, Jiahuan Zhou, Danyang Zhang, Jiwen Lu, Ying Wu, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
	      <br>
	      <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2006.07665">[arxiv]</a> 
              <a href="https://github.com/nzl-thu/MUSDL">[Code]</a>
              <br>
              <p> We propose an uncertainty-aware score distribution learning method and extend it to a multi-path model for action quality assessment.</p>
            </td>
          </tr>
		  
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/tpami.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2020
	      <br>
              <a href="https://arxiv.org/abs/2003.09392">[arXiv]</a>  
	      <a href="https://coin-dataset.github.io/">[Project Page]</a> 
              <br>
              <p> Journal version of the COIN dataset.</p>
            </td>
          </tr>
		  
		    <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TIP19.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Semantics-Preserving Attention and Contextual Interaction for Group Activity Recognition</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Jiwen Lu, Zian Wang, Ming Yang, and Jie Zhou
              <br>
              <em>IEEE Transaction on Image Processing (TIP)</em>, 2019
              <br>
              <a href="data/TIP19b.pdf">[PDF]</a>  
	      <a href="data/TIP19b_supp.pdf">[Supp]</a>  
              <br>
              <p> We extend of our Semantics-Preserving Attention model with graph convolutional module for group activity recognition.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/coin.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis</papertitle>
              <br>
              <strong>Yansong Tang</strong>, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1903.02874">[arXiv]</a>  <a href="https://coin-dataset.github.io/">[Project Page]</a>  <a href="https://github.com/coin-dataset/annotation-tool">[Annotation Tool]</a> 
              <br>
              <p> COIN is one of the largest and most comprehensive instructional video analysis datasets with rich annotations. </p>
            </td>
          </tr>	  

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/mdnn.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Multi-stream Deep Neural Networks for RGB-D Egocentric Action Recognition</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Zian Wang, Jiwen Lu, Jianjiang Feng, and Jie Zhou
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2019
	      <br>
              <a href="data/TCSVT18_MDNN.pdf">[PDF]</a>  
	      <a href="http://ivg.au.tsinghua.edu.cn/dataset/THU_READ.php">[Project Page]</a>
	      <a href="data/MDNN-TSN.tar.gz">[Code]</a>    
              <br>
              <p> We propose a multi-stream deep neural networks and THU-READ dataset for RGB-D egocentric action recognition.</p>
            </td>
          </tr>		
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/spts.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Mining Semantics-Preserving Attention for Group Activity Recognition</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Zian Wang, Peiyang Li, Jiwen Lu, Ming Yang, and Jie Zhou
              <br>
              <em>ACM Multimedia (MM)</em>, 2018
		    	      <br>
			  <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2003.09392">[PDF]</a>     
              <br>
              <p> We present a simple yet effective semantics-preserving attention module for group activity recognition.</p>
            </td>
          </tr>			
		
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/dprl.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition</papertitle>
              <br>
			  <strong>Yansong Tang*</strong>, Yi Tian*, Jiwen Lu, Peiyang Li, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018
              <a href="data/CVPR_DPRL_paper.pdf">[PDF]</a>  
              <br>
              <p> We propose a simple yet effective method to select key frames for skeleton-based action recognition using the REINFORCE algorithm.</p>
            </td>
          </tr>
		
		
        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Honors and Awards</heading>
            <p>
			  <li style="margin: 5px;" >
                Excellent PhD Graduate of Beijing, 2020.
              </li>
			  <li style="margin: 5px;" >
                Excellent Doctoral Dissertation of Tsinghua University, 2020.
              </li>
			  <li style="margin: 5px;" >
                Zijing Scholar Fellowship for Prospective Researcher, Tsinghua University, 2020.
              </li>
			  <li style="margin: 5px;" >
                National Scholarship, Tsinghua University, 2018.
              </li>
			  <li style="margin: 5px;" >
                Outstanding Student Cadres, Tsinghua University, 2015.
              </li>		    
			  <li style="margin: 5px;" >
                EMC Integrated Merit Scholarship, Tsinghua University, 2014.
              </li>		    
			  <li style="margin: 5px;" >
                Zhang-Mingwei Scholarship, Tsinghua University, 2013.
              </li>		    
			  <li style="margin: 5px;" >
                MC Integrated Merit Scholarship, Tsinghua University, 2012.
              </li>			    
		    
		    
            </p>
          </td>
        </tr>
      </tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR 2019/2020, ICCV 2019, ECCV 2020, AAAI 2020/2021, ICME 2019/2020/2021, ICIP 2018/2019
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  TPAMI, TIP, TMM, TCSVT
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
<a href="https://clustrmaps.com/site/1bevc" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=pSqGxdyauRrVjsHq2YSTApeSzMqOewuzLgOt0g6Ow6Y&cl=ffffff" /></a>	          
       
      </td>
    </tr>
  </table>
</body>

</html>
