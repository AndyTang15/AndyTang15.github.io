<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yansong Tang</title>
  
  <meta name="author" content="Yansong Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:820px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yansong Tang</name>
              </p>
              <p>I am a tenure-track Assistant Professor of <a href="https://www.tbsi.edu.cn/en/main.psp">Tsinghua-Berkeley Shenzhen Institute</a>, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, where I direct the IVG@SZ (Intelligent Vision Group at Shenzhen, the sister group of the <a href="http://ivg.au.tsinghua.edu.cn/"> IVG </a> at Beijing). Before that, I was a postdoctoral researcher at the Department of Engineering Science of the University of Oxford, working with <a href="https://www.robots.ox.ac.uk/~phst/"> Prof. Philip H. S. Torr</a> and <a href="https://www.robots.ox.ac.uk/~victor/"> Prof. Victor Prisacariu</a>. My research interests lie in computer vision. Currently, I am working in the fields of video analytics, vision-language understanding and 3D reconstruction.</p>
	      <p>I received my Ph.D degree with honour at Tsinghua University, advised by <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en"> Prof. Jie Zhou </a> and <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Prof. Jiwen Lu</a>, and B.S. degree in Automation from Tsinghua University. I have also spent time at <a href="https://www.microsoft.com/en-us/research/group/visual-computing/"> Visual Computing Group </a> of Microsoft Research Asia (MSRA), and <a href="http://www.stat.ucla.edu/~sczhu/"> Prof. Song-Chun Zhu</a>’s VCLA lab of University of California, Los Angeles (UCLA).</p>
	      <p>I am looking for self-motivated Master/PhD/Postdoc. If you have top grades or coding skill, and are highly creative and interested in joining my group, please do not hesitate to send me your CV and transcripts of grades by <a href="data/email.txt">Email </a>.</p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
	      <img style="width:80%;max-width:80%" alt="profile photo" src="images/yansongtang.jpg">
            </td>
          </tr>
        </tbody></table>
		
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" >
                <b>2022-07:</b> Two papers to appear in ECCV 2022.
              </li>
              <li style="margin: 5px;" >
                <b>2022-04:</b> A talk at MSRA about <a href="data/LAVT-Yansong.pdf">LAVT</a>.
	      </li>
              <li style="margin: 5px;" >
                <b>2022-03:</b> Five papers to appear in CVPR 2022.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent Selected Publications  [<a href="https://scholar.google.com/citations?user=TIbistUAAAAJ&hl=zh-CN"> Full List </a>]</heading>
			  <p>(*Equal Contribution, #Corresponding Author)</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/hornet.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions</papertitle>
              <br>
              <a href="https://raoyongming.github.io/"> Yongming Rao</a>*, 
              <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>*, 
              <a href="https://andytang15.github.io/"> Yansong Tang</a>, 
	      <strong>Yansong Tang</strong>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="https://sites.google.com/site/sernam"> Ser-Nam Lim </a>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>Preprint</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2207.14284">[arXiv]</a>
              <a href="https://github.com/raoyongming/HorNet">[Code]</a>
              <a href="https://hornet.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/MyMIPv-bn9wVMLABurjOUA">[中文解读]</a> 
              <br>
              <p> HorNet is a family of generic vision backbones that perform explicit high-order spatial interactions based on Recursive Gated Convolution.</p>
            </td>
          </tr>		
		
		
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/lavt.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LAVT: Language-Aware Vision Transformer for Referring Image Segmentation</papertitle>
              <br>
		    Zhao Yang*, Jiaqi Wang*, <strong>Yansong Tang#</strong>, Kai Chen, Hengshuang Zhao, Philip H.S. Torr
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2112.02244v2.pdf">[arxiv]</a>
	      <a href="https://github.com/yz93/LAVT-RIS">[code]</a>
              <br>
              <p> We present an end-to-end hierarchical Transformer-based network for referring segmentation.</p>
            </td>
          </tr>		

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BNV-Fusion.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion</papertitle>
              <br>
		    Kejie Li, <strong>Yansong Tang</strong>, Victor Adrian Prisacariu, Philip H.S. Torr
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2204.01139">[arxiv]</a>   
    	      <a href="https://github.com/likojack/bnv_fusion">[code]</a>
              <br>
              <p> We present Bi-level Neural Volume Fusion, which leverages recent advances in neural implicit representations and neural rendering for dense 3D reconstruction. In order to incrementally integrate new depth maps into a global neural implicit representation, we propose a novel bi-level fusion strategy that considers both efficiency and reconstruction quality by design.</p>
            </td>
          </tr>			
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/denseclip.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</papertitle>
              <br>
		    Yongming Rao*, Wenliang Zhao*, Guangyi Chen, <strong>Yansong Tang</strong>, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2112.01518">[arxiv]</a>
              <a href="https://github.com/raoyongming/DenseCLIP">[code]</a>  	    
              <br>
              <p> DenseCLIP is a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP.</p>
            </td>
          </tr>				
		
	
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/tpami.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation</papertitle>
              <br>
			  <strong>Yansong Tang</strong>, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2021
	      <br>
              <a href="https://arxiv.org/abs/2003.09392">[arXiv]</a>  
	      <a href="https://coin-dataset.github.io/">[Project Page]</a> 
              <br>
              <p> COIN is currently the largest and most comprehensive instructional video analysis datasets with rich annotations.</p>
            </td>
          </tr>			

	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/musdl.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Uncertainty-aware Score Distribution Learning for Action Quality Assessment</papertitle>
              <br>
			  <strong>Yansong Tang*</strong>, Zanlin Ni*, Jiahuan Zhou, Danyang Zhang, Jiwen Lu, Ying Wu, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
	      <br>
	      <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2006.07665">[arxiv]</a> 
              <a href="https://github.com/nzl-thu/MUSDL">[Code]</a>
              <br>
              <p> We propose an uncertainty-aware score distribution learning method and extend it to a multi-path model for action quality assessment.</p>
            </td>
          </tr>
		
		
        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Honors and Awards</heading>
            <p>
			  <li style="margin: 5px;" >		    
		    Excellent Doctoral Dissertation Award of CAAI, 2021.
              </li>		    
			  <li style="margin: 5px;" >
                Excellent PhD Graduate of Beijing, 2020.
              </li>
			  <li style="margin: 5px;" >
                Excellent Doctoral Dissertation of Tsinghua University, 2020.
              </li>
			  <li style="margin: 5px;" >
                Zijing Scholar Fellowship for Prospective Researcher, Tsinghua University, 2020.
              </li>  
            </p>
          </td>
        </tr>
      </tbody></table>

	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Group</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>PhD Students:</b>
	              <table  border=3 width = 1200  bordercolor="white" align="left">
                  <tr>
                  <td width= "400">Zhiheng Li (2021-; with <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>)</td>
                  <td width= "400">Yixuan Zhu (2022-; with <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>)</td>
                  <td width= "400"></td>
                  </tr>
                </table> 
              </li>
              <li style="margin: 5px;"> 
                <b>Master Students:</b>
                <table  border=3   width = 1200 bordercolor="white" align="left">
                  <tr>
                  <td width= "400"><a href="https://xk-huang.github.io">Xiaoke Huang</a> (2021-; with <a href="https://scholar.google.com/citations?user=TN8uDQoAAAAJ&hl=en">Prof. Jiwen Lu</a>)</td>
                  <td width= "400">Rong He (2021-; with <a href="https://scholar.google.com/citations?user=TN8uDQoAAAAJ&hl=en">Prof. Jiwen Lu</a>)</td>
                  <td width= "400">Yiji Cheng (2022-)</td>
	          <td width= "400">Jinpeng Liu (2022-)</td>	
		  <td width= "400">Aoyang Liu (2022-)</td>
		  <td width= "400">Sujia Wang (2022-)</td>		  
		  <td width= "400">Yunzhi Teng (2022-)</td>
		  <td width= "400">Wenjia Geng (2022-; with <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>)</td>		   
                  <td width= "400"></td>
                  </tr>
                </table> 
	       </li>		    
            </p>
          </td>
        </tr>
      </tbody></table>	      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Group</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>PhD Students:</b>
                <p>Zhiheng Li (2021-; with <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>)</p>
                <p>Yixuan Zhu (2022-; with <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Prof. Jie Zhou</a>)</p>		      
              </li>
              <li style="margin: 5px;"> 
                <b>Master Students:</b>
                <p><a href="https://xk-huang.github.io">Xiaoke Huang</a> (2021-; with <a href="https://scholar.google.com/citations?user=TN8uDQoAAAAJ&hl=en">Prof. Jiwen Lu</a>)</p>
                <p>Rong He (2021-; with <a href="https://scholar.google.com/citations?user=TN8uDQoAAAAJ&hl=en">Prof. Jiwen Lu</a>)</p>
	       </li>		    
            </p>
          </td>
        </tr>
      </tbody></table>
	      
	      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Area Chair:</b> FG 2023
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR, ICCV, ECCV, AAAI and so on
              </li>		    
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  TPAMI, TIP, TMM, TCSVT and so on
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
<a href="https://clustrmaps.com/site/1bevc" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=pSqGxdyauRrVjsHq2YSTApeSzMqOewuzLgOt0g6Ow6Y&cl=ffffff" /></a>	          
       
      </td>
    </tr>
  </table>
</body>

</html>
