// Multimodal understanding
const multimodalUnderstanding = [
    { title: "FLAG3D++: A Benchmark for 3D Fitness Activity Comprehension With Language Instruction", venue: "TPAMI 2025" },
    { title: "Learning High-Quality Dynamic Memory for Video Object Segmentation", venue: "TPAMI 2025" },
    { title: "Language-Aware Vision Transformer for Referring Segmentation", venue: "TPAMI 2024" },
    { title: "Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation", venue: "TPAMI 2021" },
    { title: "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams", venue: "ICCV 2025" },
    { title: "Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation", venue: "ICCV 2025" },
    { title: "VoCo-LLaMA: Towards Vision Compression with Large Language Models", venue: "CVPR 2025" },
    { title: "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models", venue: "CVPR 2025" },
    { title: "SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes", venue: "CVPR 2025" },
    { title: "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct", venue: "ICLR 2025" },
    { title: "Segment and Caption Anything", venue: "CVPR 2024" },
    { title: "Universal Segmentation at Arbitrary Granularity with Language Instruction", venue: "CVPR 2024" },
    { title: "Open-Vocabulary Segmentation with Semantic-Assisted Calibration", venue: "CVPR 2024" },
    { title: "Narrative Action Evaluation with Prompt-Guided Multimodal Interaction", venue: "CVPR 2024" },
    { title: "MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory", venue: "NeurIPS 2023" },
    { title: "Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer", venue: "ICCV 2023" },
    { title: "Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning", venue: "ICCV 2023" },
    { title: "FLAG3D: A 3D Fitness Activity Dataset with Language Instruction", venue: "CVPR 2023" },
    { title: "LOGO: A Long-Form Video Dataset for Group Action Quality Assessment", venue: "CVPR 2023" },
    { title: "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions", venue: "NeurIPS 2022" },
    { title: "OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression", venue: "NeurIPS 2022" },
    { title: "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation", venue: "CVPR 2022" },
    { title: "DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting", venue: "CVPR 2022" },
    { title: "Uncertainty-aware Score Distribution Learning for Action Quality Assessment", venue: "CVPR 2020" },
    { title: "Q-VLM: Post-training Quantization for Large Vision-Language Models", venue: "NeurIPS 2024" }
];

// Visual generation
const visualGeneration = [
    { title: "Efficient Text-Guided 3D-Aware Generation with Score Distillation on 3D Distribution", venue: "TCSVT 2025" },
    { title: "Stableswap: Stable face swapping in a shared and controllable latent space", venue: "TMM 2024" },
    { title: "KV-Edit: Training-Free Image Editing for Precise Background Preservation", venue: "ICCV 2025" },
    { title: "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios", venue: "SIGGRAPH 2025" },
    { title: "FADE: Frequency-Aware Diffusion Model Factorization for Video Editing", venue: "CVPR 2025" },
    { title: "InstaRevive: One-Step Image Enhancement via Dynamic Score Matching", venue: "ICLR 2025" },
    { title: "GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D Gaussian Generation", venue: "NeurIPS 2024" },
    { title: "Towards Accurate Data-free Quantization for Diffusion Models", venue: "CVPR 2024" },
    { title: "FlowIE: Efficient Image Enhancement via Rectified Flow", venue: "CVPR 2024" },
    { title: "MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model", venue: "ECCV 2024" },
    { title: "Plan, Posture and Go: Towards Open-Vocabulary Text-to-Motion Generation", venue: "ECCV 2024" },
    { title: "DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery", venue: "CVPR 2024" }
];

// Embodied Agent
const embodiedAgent = [
    { title: "OccNeRF: Advancing 3D Occupancy Prediction in LiDAR-Free Environments", venue: "TIP 2025" },
    { title: "Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large Scene Reconstruction", venue: "ICCV 2025" },
    { title: "ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion", venue: "ICCV 2025" },
    { title: "AnyBimanual: Transferring Unimanual Policy for General Bimanual Manipulation", venue: "ICCV 2025" },
    { title: "ManiGaussian++: Dynamic Gaussian Splatting for Multi-task Bimanual Manipulation", venue: "IROS 2025" },
    { title: "ThinkBot: Embodied Instruction Following with Thought Chain Reasoning", venue: "ICLR 2025" },
    { title: "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation", venue: "ECCV 2024" },
    { title: "BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion", venue: "CVPR 2022" }
];
